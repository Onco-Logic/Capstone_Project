{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ScMmp0UnwJlv",
   "metadata": {
    "id": "ScMmp0UnwJlv"
   },
   "source": [
    "# **Onco-Logic - Robot Oncologist1**\n",
    "---\n",
    "## **Team Members**\n",
    "*   Rahim Siddiq (risiddiq@csumb.edu)\n",
    "*   Harris Popal (hapopal@csumb.edu)\n",
    "*   Federico Marquez Murrieta (fedmarquezmurrieta@csumb.edu)\n",
    "*   Nicholas Marolla (nmarolla@csumb.edu)\n",
    "\n",
    "---\n",
    "## **Project Overview**\n",
    "Patients often struggle to interpret their symptoms and lab test results, leading to delayed medical consultations or reliance on unreliable online sources. This project aims to bridge this gap by providing an AI-powered medical chatbot that assists users in understanding symptoms and interpreting blood test results.\n",
    "\n",
    "---\n",
    "## **Selection of Data**\n",
    "\n",
    "1. **Medical Chatbot:**  \n",
    "    - Using the `patient_doctor_chatbot` dataset, we predict diagnostic clusters based on patient symptom descriptions and simulate a real-life consultation experience.\n",
    "    - Contains 154,150 real-world patient-doctor dialogues, including symptoms, diagnoses, and treatment suggestions.\n",
    "2. **Medical Disease Prediction:**  \n",
    "   - blood test biomarkers for various diseases (Diabetes, Anemia, Thrombosis).\n",
    "3. **Survival Prediction:**  \n",
    "   - Leveraging data from UC Davis, we predict patient survival outcomes by analyzing disease type, demographic factors, and survival probability trends over time. The model estimates 5-year, 10-year, and 15-year survival rates, assisting in long-term health assessments.\n",
    "\n",
    "### **Data Processing**:\n",
    "- **Textual Data (Chatbot)**\n",
    "\n",
    "  Preprocessing: Removal of stopwords, named entity recognition (NER) for symptom extraction.Feature Engineering: Uses Sentence-BERT embeddings to represent symptom descriptions and retrieve similar cases. Cleaning textual data, encoding diagnosis labels, and ensuring balanced splits between training and test sets.\n",
    "\n",
    "- **Numerical Data (Biomarkers)**\n",
    "\n",
    "  Handling Missing Values: Imputation using median values.\n",
    "  Feature Selection: Identifying the most relevant biomarkers for disease prediction.\n",
    "\n",
    "- **Numerical Data (Biomarkers & Survival Data)**\n",
    "\n",
    "  Handling Missing Values: Imputation using median values for missing biomarkers and patient characteristics.\n",
    "  Feature Engineering: Standardization and normalization of numerical variables.\n",
    "  Feature Selection: Identifying the most relevant biomarkers using SHAP values and recursive feature elimination.\n",
    "  Data Splitting: Ensuring balanced training and test sets for survival analysis and disease classification.\n",
    "\n",
    "---\n",
    "## **Methods Technologies and Tools**\n",
    "-  Programming Languages: Python (Pandas, Numpy, Scikit-learn, TensorFlow, PyTorch)\n",
    "-  NLP Model: Sentence-BERT (SBERT)\n",
    "-  Machine Learning Algorithms: Logistic Regression, Random Forest, CatBoost, Naive Bayes\n",
    "-  APIs & Libraries: Hugging Face, Spacy (NER), Streamlit (Web App)\n",
    "-  Development Environment: Google Colab, Jupyter Notebook, Anaconda, Spyder\n",
    "\n",
    "### Model Implementation\n",
    "- Medical Chatbot (NLP)\n",
    "\n",
    "  - Sentence-BERT embeddings for similarity retrieval.\n",
    "  - Named Entity Recognition (NER) for extracting symptoms.\n",
    "  - Finds the most similar case using cosine similarity.\n",
    "  - Streamlit UI for user interaction.\n",
    "\n",
    "- Disease Prediction (ML Model)\n",
    "\n",
    "  - Feature selection from blood test biomarkers.\n",
    "  - Training multiple classifiers (Logistic Regression, Random Forest, CatBoost).\n",
    "  - Hyperparameter tuning for accuracy optimization.\n",
    "\n",
    "- Disease & Survival Prediction (ML Model)\n",
    "  \n",
    "  - Feature selection from biomarkers and survival data.\n",
    "  - Training multiple classifiers: Random Forest, Gradient Boosting, XGBoost, and Stacking Regressors.\n",
    "  - Hyperparameter tuning: GridSearchCV for optimizing classifier performance.\n",
    "  - Performance evaluation: Comparison of MAE, RMSE, and R¬≤ scores across different models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Medical Chatbot\n",
    "\n",
    "**Source:**  \n",
    "[Sohaibsoussi/patient_doctor_chatbot on HuggingFace](https://huggingface.co/datasets/Sohaibsoussi/patient_doctor_chatbot)\n",
    "\n",
    "**Description:**  \n",
    "This dataset contains patient verbal communications that are summarized into general problem statements, which are then mapped to a corresponding doctor‚Äôs diagnosis. The idea is to simulate a realistic medical consultation through a chatbot interface.\n",
    "\n",
    "**Prediction Goal:**  \n",
    "Develop a system that, given a patient‚Äôs symptom description, predicts a diagnosis cluster (or specific condition) and provides corresponding doctor's advice.\n",
    "\n",
    "**Key Features:**  \n",
    "- **Patient Descriptions:** Free-text inputs describing symptoms.  \n",
    "- **Preprocessed Text Features:**  Cleaned and embedded representations of the patient inputs using Sentence-BERT.\n",
    "- **Doctor‚Äôs Diagnosis Labels:** The target variable, which maps each patient description to a diagnosis or treatment recommendation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Medical Disease Prediction\n",
    "\n",
    "**Source:**  \n",
    "[Kaggle - Multiple Disease Prediction](https://www.kaggle.com/datasets/ehababoelnaga/multiple-disease-prediction/data)  \n",
    "Files: `Blood_samples_dataset_balanced_2.csv` and `blood_samples_dataset_test.csv`\n",
    "\n",
    "**Description:**  \n",
    "This dataset comprises detailed blood test results containing various biomarkers. The balanced training set and a separate test set are used to develop a predictive model.\n",
    "\n",
    "**Prediction Goal:**  \n",
    "Build a machine learning model to predict the underlying disease or condition based on the values of blood markers. The model will analyze multiple biomarkers from blood test results to interpret and forecast the patient's health status.\n",
    "\n",
    "**Key Features:**  \n",
    "- **Blood Marker Values:** Quantitative measurements from blood tests (e.g., levels of hemoglobin, white blood cells, etc.).  \n",
    "- **Additional Clinical Variables:** If available, demographic or clinical factors that may influence the prediction.\n",
    "\n",
    "---\n",
    "## 3. Survival Prediction\n",
    "\n",
    "**Source:**  \n",
    "[Survival and relapse in patients with thrombotic thrombocytopenic purpura](https://pubmed.ncbi.nlm.nih.gov/20032506/)\n",
    "\n",
    "\n",
    "[Improved survival in both men and women with diabetes between 1980 and 2004 ‚Äì a cohort study in Sweden](https://pmc.ncbi.nlm.nih.gov/articles/PMC2586621/)\n",
    "\n",
    "\n",
    "[The survival rate of patients with beta-thalassemia major and intermedia and its trends in recent years in Iran](https://pmc.ncbi.nlm.nih.gov/articles/PMC6335498/)\n",
    "\n",
    "\n",
    "[Incidence and outcome of acquired aplastic anemia: real-world data from patients diagnosed in Sweden from 2000-2011](https://pubmed.ncbi.nlm.nih.gov/28751565/)\n",
    "\n",
    "\n",
    "Files: `survival_data.csv` and\n",
    "`Blood_samples_dataset_balanced_2.csv`, and\n",
    "`blood_samples_dataset_test.csv`\n",
    "\n",
    "\n",
    "**Description:**  \n",
    "This dataset contains survival data for patients with various medical conditions, segmented by disease type, gender, and age group. The dataset includes survival probabilities at 5-year, 10-year, and 15-year intervals, enabling predictive modeling for long-term health outcomes.\n",
    "\n",
    "\n",
    "**Prediction Goal:**  \n",
    "Develop a machine learning model to predict survival rates based on disease type, demographic factors, and key biomarkers. The model will assess patient risk and estimate long-term survival probabilities, aiding in prognosis and decision-making.\n",
    "\n",
    "\n",
    "**Key Features:**  \n",
    "- **Disease Type:** Includes survival data for Diabetes, Thalassemia, Aplastic Anemia, and Thrombotic Thrombocytopenic Purpura (TTP).  \n",
    "- **Demographics:** Gender and age group segmentation (e.g., 0-40, 40-60, 60+) to analyze survival variations.  \n",
    "- **Survival Probabilities:**\n",
    "5-Year Survival Rate: Short-term survival likelihood.\n",
    "10-Year Survival Rate: Mid-term prognosis estimation.\n",
    "15-Year Survival Rate: Long-term survival projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-X8CG884jeWy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18232,
     "status": "ok",
     "timestamp": 1747716469695,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "-X8CG884jeWy",
    "outputId": "fba7ddac-8e8e-4484-96e9-1e661395b44d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WNBDYj3OHf6d",
   "metadata": {
    "id": "WNBDYj3OHf6d"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xC6xtSYH_cb3",
   "metadata": {
    "id": "xC6xtSYH_cb3"
   },
   "source": [
    "### Using pydrive to access shared folder and download project files into colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xq0Rt8Hx2aaY",
   "metadata": {
    "id": "Xq0Rt8Hx2aaY"
   },
   "outputs": [],
   "source": [
    "!pip install -q pydrive2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afeUjhBZ3hI",
   "metadata": {
    "id": "5afeUjhBZ3hI"
   },
   "source": [
    "### Authenticate user with pydrive in order to access shared project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mTb4WlKe2aWQ",
   "metadata": {
    "id": "mTb4WlKe2aWQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lcopSXa22aRP",
   "metadata": {
    "id": "lcopSXa22aRP"
   },
   "outputs": [],
   "source": [
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Initialize GoogleAuth and set credentials\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "# Initialize GoogleDrive with the authenticated GoogleAuth instance\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yG-tgeNWYCIg",
   "metadata": {
    "id": "yG-tgeNWYCIg"
   },
   "source": [
    "### Check access to shared folder for the project in Google Drive and list files and folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4kdJ4elA2aKe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1747716488240,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "4kdJ4elA2aKe",
    "outputId": "9ed388c3-b66f-4069-abff-5932563dbb41"
   },
   "outputs": [],
   "source": [
    "folder_id = '18vnDLs39XXMKyqdfVeFCMHtehOWpnjnL'\n",
    "\n",
    "# List all files in the specified folder\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"Title: {file['title']}, ID: {file['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txlS3-iBYkWh",
   "metadata": {
    "id": "txlS3-iBYkWh"
   },
   "source": [
    "### Download all the files ending with .csv, .ipynb, and .py into the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8AYeaRsw2Z_B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16733,
     "status": "ok",
     "timestamp": 1747716504974,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "8AYeaRsw2Z_B",
    "outputId": "b5ff5e2e-86bf-4aa0-e6a5-40a6547e8d90"
   },
   "outputs": [],
   "source": [
    "filtered_files = [file for file in file_list if file['title'].endswith('.csv') or file['title'].endswith('.ipynb') or file['title'].endswith('.py')]\n",
    "\n",
    "for file in filtered_files:\n",
    "    print(f\"‚¨áÔ∏è Downloading: {file['title']} ...\")\n",
    "    file.GetContentFile(file['title'])  # Save file in Colab\n",
    "\n",
    "print(\"‚úÖ Download completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fw6KLWCgZC-s",
   "metadata": {
    "id": "fw6KLWCgZC-s"
   },
   "source": [
    "### Make a directory called pages and pull in all the .py scripts from the pages folder for the project from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yRbGnUDESbNF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3987,
     "status": "ok",
     "timestamp": 1747716508970,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "yRbGnUDESbNF",
    "outputId": "47231eeb-182c-432c-e03b-017d218de2f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subfolder = \"pages\"\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "pages_folder_id = '1SAK2-zTlgTDqdhoWjsCq8WNn0Vva3au3'\n",
    "\n",
    "pages_file_list = drive.ListFile({\n",
    "    'q': f\"'{pages_folder_id}' in parents and trashed=false\"\n",
    "}).GetList()\n",
    "\n",
    "print(\"Files in the 'pages' folder on Drive:\")\n",
    "for file in pages_file_list:\n",
    "    print(f\"Title: {file['title']}, ID: {file['id']}\")\n",
    "\n",
    "for file in pages_file_list:\n",
    "    destination = os.path.join(subfolder, file['title'])\n",
    "    print(f\"‚¨áÔ∏è Downloading: {file['title']} to {destination} ...\")\n",
    "    file.GetContentFile(destination)\n",
    "\n",
    "print(\"‚úÖ Download completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DHrg1ISM_0lE",
   "metadata": {
    "id": "DHrg1ISM_0lE"
   },
   "source": [
    "### Creating directories for training models and directories to house trained models to speed up run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YI0xZBw3L1L0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3980,
     "status": "ok",
     "timestamp": 1747716512952,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "YI0xZBw3L1L0",
    "outputId": "ded57af7-c241-4031-e069-b88cdbeb3b8f"
   },
   "outputs": [],
   "source": [
    "subfolder1 = \"models\"\n",
    "if not os.path.exists(subfolder1):\n",
    "    os.makedirs(subfolder1)\n",
    "\n",
    "subfolder2a = \"models/models\"\n",
    "if not os.path.exists(subfolder2a):\n",
    "    os.makedirs(subfolder2a)\n",
    "\n",
    "subfolder2ai = \"models/models/data\"\n",
    "if not os.path.exists(subfolder2ai):\n",
    "    os.makedirs(subfolder2ai)\n",
    "\n",
    "subfolder2b = \"models/catboost_info\"\n",
    "if not os.path.exists(subfolder2b):\n",
    "    os.makedirs(subfolder2b)\n",
    "\n",
    "subfolder2bi = \"models/catboost_info/learn\"\n",
    "if not os.path.exists(subfolder2bi):\n",
    "    os.makedirs(subfolder2bi)\n",
    "\n",
    "models_folder_id = '1tzl4IrKarl_yRGHkR4vzDQuicGPiXRkf'\n",
    "model_files = drive.ListFile({'q': f\"'{models_folder_id}' in parents and trashed=false\"}).GetList()\n",
    "filtered_model_files = [file for file in model_files if file['title'].endswith('.py')]\n",
    "print(filtered_model_files)\n",
    "for file in filtered_model_files:\n",
    "    destination1 = os.path.join(subfolder1, file['title'])\n",
    "    print(f\"‚¨áÔ∏è Downloading: {file['title']} ...\")\n",
    "    file.GetContentFile(destination1)\n",
    "\n",
    "modeld_data_id = '1WP3ZypQ8Z-79PeZJ4Ta_D2cBCWI7F3-N'\n",
    "model_data_files = drive.ListFile({'q': f\"'{modeld_data_id}' in parents and trashed=false\"}).GetList()\n",
    "filtered_model_data_files = [file for file in model_data_files if file['title'].endswith('.csv')]\n",
    "for file in filtered_model_data_files:\n",
    "    destination2 = os.path.join(subfolder2ai, file['title'])\n",
    "    print(f\"‚¨áÔ∏è Downloading: {file['title']} ...\")\n",
    "    file.GetContentFile(destination2)\n",
    "print(\"‚úÖ Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TMYMo8Wz8Eu-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2120,
     "status": "ok",
     "timestamp": 1747716515082,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "TMYMo8Wz8Eu-",
    "outputId": "e2cb0e75-11db-48f2-8d1d-1e8765be7bbd"
   },
   "outputs": [],
   "source": [
    "folder_id = \"106Ym4Vh8AR9X9xVnTUbO-1HDcX8UJ_J-\"\n",
    "local_folder = \"/content/medschool\"\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "    if file['title'].endswith('.npy'):  # Automatically finds .npy files\n",
    "        destination = os.path.join(local_folder, file['title'])\n",
    "        print(f\"Downloading {file['title']} to {destination} ...\")\n",
    "        file.GetContentFile(destination)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tzN_RdWs-QrB",
   "metadata": {
    "id": "tzN_RdWs-QrB"
   },
   "source": [
    "### Download dependencies needed throughout the project into the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctBbvzGgOeoZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 37806,
     "status": "ok",
     "timestamp": 1747716552896,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "ctBbvzGgOeoZ",
    "outputId": "b8accfd5-7e84-4d2a-e243-e34a44e8a6fd"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit -q\n",
    "!pip install nltk -q\n",
    "!pip install spacy -q\n",
    "!pip install pyngrok -q\n",
    "!pip install catboost -q\n",
    "!pip install supertree -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46c8f6-d657-454b-9e65-9dc896f421d0",
   "metadata": {
    "id": "4e46c8f6-d657-454b-9e65-9dc896f421d0"
   },
   "source": [
    "### Train models required by scripts in pages subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "INFV8IF8sxfn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 136078,
     "status": "ok",
     "timestamp": 1747716688981,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "INFV8IF8sxfn",
    "outputId": "cd828f47-bf64-4df9-df12-3fd9125ba9b0"
   },
   "outputs": [],
   "source": [
    "%run models/train_models.py\n",
    "%run models/train_survival_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "em_5-ptCbi-s",
   "metadata": {
    "id": "em_5-ptCbi-s"
   },
   "source": [
    "### Import statements for medical_chatbot. Using magic commands to write code to script for streamlit user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbedcb-bc70-475a-b2bd-410833ff8d1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716688989,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "79dbedcb-bc70-475a-b2bd-410833ff8d1e",
    "outputId": "45452d92-38f9-49a1-c71e-3e32a0818241"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GVZ-ERBSb00o",
   "metadata": {
    "id": "GVZ-ERBSb00o"
   },
   "source": [
    "### Download dependencies from Natural Language Toolkit (nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054eba2-389a-4c77-86d4-a96021b7bc12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747716688997,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "5054eba2-389a-4c77-86d4-a96021b7bc12",
    "outputId": "c754691e-0aba-4d9d-bedc-cd2524905bf1"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1LuP9docARq",
   "metadata": {
    "id": "e1LuP9docARq"
   },
   "source": [
    "### Spacy Natural Language Processing (nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LpszQ2-ZN2Ps",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716689003,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "LpszQ2-ZN2Ps",
    "outputId": "72b3f054-1967-4595-a5e7-8a300dd3a13c"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39tRD-uxcVW1",
   "metadata": {
    "id": "39tRD-uxcVW1"
   },
   "source": [
    "### Loading Dataframe from small sample of original dataset.\n",
    "This is for demonstration purposes, due to compute limitations the original dataset of 150k rows is trimmed to 10k and processed locally. Sample only contains a few hundred rows to demonstrate how processing was accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6123a23-d708-4eb4-bb2b-4c68674cbb82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716689009,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "e6123a23-d708-4eb4-bb2b-4c68674cbb82",
    "outputId": "0868dd31-4428-4d1e-9682-bcd15da90452"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "# Small sample dataset\n",
    "df = pd.read_csv('/content/train_data3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pih9WkDydhz-",
   "metadata": {
    "id": "Pih9WkDydhz-"
   },
   "source": [
    "### Pre-processing and cleaning of 'Description' column. Duplicate patient data is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ud0gwH6IbmBF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716689015,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "Ud0gwH6IbmBF",
    "outputId": "e894e1cb-2995-40ab-8c82-e26da59c243b"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "df['Description'] = df['Description'].str.replace(r'^Description:\\s*', '', regex=True)\n",
    "df['Description'] = df['Description'].str.replace(r'^Q\\.\\s*', '', regex=True)\n",
    "df = df[['Patient', 'Description', 'Doctor']]\n",
    "df = df.drop_duplicates(subset=['Patient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rs-f6taJd4Dy",
   "metadata": {
    "id": "rs-f6taJd4Dy"
   },
   "source": [
    "### Pre-compile regex patterns needed to clean text and build stop words set combining NLTK stopwords with additonal words to screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YcxVFEpab0cg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747716689020,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "YcxVFEpab0cg",
    "outputId": "ba6e6f06-223b-4cd1-b86d-cf9cb4af21b8"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "number_pattern = re.compile(r'\\d+')\n",
    "special_char_pattern = re.compile(r'\\W+')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english')).union({\n",
    "    'hi', 'hello', 'doctor', 'year', 'old', 'thanks', 'yrs', 'suggest', 'remedy', 'treatment',\n",
    "    'years', 'patient', 'patients', 'dear', 'sir', 'thank', 'cure', 'reason', 'treated', 'cause',\n",
    "    'age', 'name', 'doc', 'please', 'help', 'causes', 'suggestion', 'could', 'want', 'treat'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WpAz-beDeTPp",
   "metadata": {
    "id": "WpAz-beDeTPp"
   },
   "source": [
    "### Text processing functions.\n",
    "*   def clean_text(text): Lowercase the text, remove numbers and special characters, and filter out common stopwords.\n",
    "*   def deduplicate_tokens(text): Remove duplicate tokens.\n",
    "*   def extract_medical_entities(text): Extracts medical entities (e.g., symptoms, diseases) from text using spaCy's NER.\n",
    "    *   If no entities fallback to cleantext\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6zHXfoyUBAzY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747716689027,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "6zHXfoyUBAzY",
    "outputId": "97312e91-1911-4222-936f-d1254f29548f"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = number_pattern.sub('', text)  # Remove numbers\n",
    "    text = special_char_pattern.sub(' ', text)  # Remove special characters\n",
    "    words = (w for w in text.split() if w not in STOP_WORDS)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def deduplicate_tokens(text):\n",
    "    tokens = text.split()\n",
    "    return \" \".join(dict.fromkeys(tokens))\n",
    "\n",
    "def extract_medical_entities(text):\n",
    "    doc = nlp(text)\n",
    "    medical_labels = {\"DISEASE\", \"SYMPTOM\", \"CONDITION\", \"MEDICATION\"}  # Define entity labels of interest\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in medical_labels]\n",
    "    extracted = \" \".join(entities) if entities else clean_text(text)  # Fallback to cleaned text if no entities\n",
    "    return deduplicate_tokens(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3IVGfNUsfpO8",
   "metadata": {
    "id": "3IVGfNUsfpO8"
   },
   "source": [
    "### Apply text cleaning and entity extraction to the dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295a949-d7d9-4bf3-b768-56b72ad2128b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716689033,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "f295a949-d7d9-4bf3-b768-56b72ad2128b",
    "outputId": "c3a9568c-ce84-4ee8-ae38-5667ba06bc15"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "df['Clean_Symptoms'] = df['Patient'].apply(extract_medical_entities)\n",
    "df['Clean_Description'] = df['Description'].apply(extract_medical_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5TAN6Sf6Vl",
   "metadata": {
    "id": "dc5TAN6Sf6Vl"
   },
   "source": [
    "### Import larger set processed with local resources using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5elENzM4gaiK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747716689038,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "5elENzM4gaiK",
    "outputId": "5a065c09-43e8-4647-c495-328ca9a8f036"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "df = pd.read_csv('/content/processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0gs2CEOUgnNd",
   "metadata": {
    "id": "0gs2CEOUgnNd"
   },
   "source": [
    "### Split data into train and test sets. 80% of the data is used for training (retrieval storage). 20% is used for testing (querying the retrieval system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08ecca-3d14-4387-b3d6-9e210c8b519e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1747716689054,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "ab08ecca-3d14-4387-b3d6-9e210c8b519e",
    "outputId": "e444285f-324b-4ec4-f05a-f0114addac31"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l5VqRL_7h34i",
   "metadata": {
    "id": "l5VqRL_7h34i"
   },
   "source": [
    "### Load Sentence-BERT(SBERT) model specifically trained for medical domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d37653-f596-4250-8030-5b55a4339bb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747716689055,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "97d37653-f596-4250-8030-5b55a4339bb5",
    "outputId": "02ea9c62-58f9-4019-837d-e7741865d3f4"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "@st.cache_resource\n",
    "def load_sbert_model():\n",
    "    # Load the medical-specific SentenceTransformer model.\n",
    "    return SentenceTransformer('pritamdeka/S-PubMedBERT-MS-MARCO')\n",
    "\n",
    "sbert_model = load_sbert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3l8sDFxWiHXP",
   "metadata": {
    "id": "3l8sDFxWiHXP"
   },
   "source": [
    "### Use SBERT to convert medical descriptions into numerical vectors for similarity comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AegzOV6eYr2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747716689055,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "5AegzOV6eYr2",
    "outputId": "f249a573-cbea-47b0-b41d-95b96ac05419"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "@st.cache_data\n",
    "def get_train_embeddings():\n",
    "    embedding_file = \"/content/medschool/train_desc_embeddings.npy\"\n",
    "    if os.path.exists(embedding_file):\n",
    "        st.write(\"Loading precomputed embeddings from file...\")\n",
    "        embeddings = np.load(embedding_file)\n",
    "    else:\n",
    "        st.write(\"Computing embeddings, this may take a while...\")\n",
    "        train_desc_texts = train_df['Clean_Description'].tolist()\n",
    "        embeddings = sbert_model.encode(train_desc_texts, show_progress_bar=False)\n",
    "        np.save(embedding_file, embeddings)\n",
    "        st.write(f\"Embeddings saved to {embedding_file}\")\n",
    "    return embeddings\n",
    "\n",
    "train_desc_embeddings = get_train_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XIoXKF1i8iz",
   "metadata": {
    "id": "3XIoXKF1i8iz"
   },
   "source": [
    "### def retrieve_best_candidate(symptoms_text, top_k=10): given symptoms text, return best matched description, doctor's response, and similarity score. This function retrieves the best-matching medical case based on a given symptom input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd08db-a068-4196-9b70-81228dfeb9e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747716689059,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "e0cd08db-a068-4196-9b70-81228dfeb9e4",
    "outputId": "a9be1d6b-ce3c-44c1-86bd-12bc97a702f2"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "@st.cache_data\n",
    "def retrieve_best_candidate(symptoms_text, top_k=10):\n",
    "    # Encode the input symptoms text.\n",
    "    input_emb = sbert_model.encode([symptoms_text], show_progress_bar=False)\n",
    "    # Compute cosine similarity between the query and all training embeddings.\n",
    "    sims = cosine_similarity(input_emb, train_desc_embeddings)[0]\n",
    "    # Retrieve indices for the top_k most similar descriptions.\n",
    "    top_k_idx = np.argsort(sims)[-top_k:][::-1]\n",
    "    best_idx = top_k_idx[0]\n",
    "    return (train_df.iloc[best_idx]['Clean_Description'],\n",
    "            train_df.iloc[best_idx]['Doctor'],\n",
    "            sims[best_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reuUbVellnHZ",
   "metadata": {
    "id": "reuUbVellnHZ"
   },
   "source": [
    "### Evaluation of the system based on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0948f0-a4a7-4944-9a5e-7bfa0ffae5f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747716689064,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "5e0948f0-a4a7-4944-9a5e-7bfa0ffae5f0",
    "outputId": "8519aa31-9361-4241-b641-d0167a7188c2"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "@st.cache_data\n",
    "def evaluate_test_set():\n",
    "    test_symptoms = test_df['Clean_Symptoms'].tolist()\n",
    "    # Batch encode all test symptoms.\n",
    "    test_emb = sbert_model.encode(test_symptoms, show_progress_bar=False)\n",
    "    # Compute cosine similarity matrix between test and training embeddings.\n",
    "    sims_matrix = cosine_similarity(test_emb, train_desc_embeddings)\n",
    "    # For each test sample, get the maximum similarity score.\n",
    "    best_similarities = np.max(sims_matrix, axis=1)\n",
    "    return best_similarities\n",
    "\n",
    "test_similarities = evaluate_test_set()\n",
    "test_df['Similarity_Score'] = test_similarities\n",
    "st.write(f\"Optimized Retrieval-Based Average Cosine Similarity: {test_df['Similarity_Score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wffU-FnrElaQ",
   "metadata": {
    "id": "wffU-FnrElaQ"
   },
   "source": [
    "### Streamlit UI for Medical Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYwDko3QEoON",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747716689069,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "MYwDko3QEoON",
    "outputId": "bd8dea59-c3f3-4986-ed49-01ff58758563"
   },
   "outputs": [],
   "source": [
    "%%writefile -a app.py\n",
    "def chatbot():\n",
    "    st.title(\"ü©∫ Medical Chatbot\")\n",
    "    st.write(\"Enter your symptoms, and I'll retrieve a similar case description along with a doctor's advice.\")\n",
    "\n",
    "    user_input = st.text_area(\"Describe your symptoms:\")\n",
    "\n",
    "    if st.button(\"Get Diagnosis\"):\n",
    "        if user_input:\n",
    "            # Extract medical entities from the user input.\n",
    "            filtered_input = extract_medical_entities(user_input)\n",
    "            # Retrieve the best candidate based on the filtered input.\n",
    "            retrieved_desc, doctor_response, similarity_score = retrieve_best_candidate(filtered_input, top_k=10)\n",
    "\n",
    "            st.subheader(\"Closest Matched Medical Description:\")\n",
    "            st.write(retrieved_desc)\n",
    "\n",
    "            st.subheader(\"Doctor's Response:\")\n",
    "            st.write(doctor_response)\n",
    "\n",
    "            st.subheader(\"Similarity Score:\")\n",
    "            st.write(f\"{similarity_score:.4f}\")\n",
    "        else:\n",
    "            st.write(\"Please enter your symptoms.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ER-oKbDVlxO8",
   "metadata": {
    "id": "ER-oKbDVlxO8"
   },
   "source": [
    "### The following two commands provide the IP for the colab runtime which is used as the tunnel password for .loca.lt link generated to run the streamlit user interface.\n",
    "### Copy IP Address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mfMs3RBLZmGu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1747716689342,
     "user": {
      "displayName": "Nicholas Marolla",
      "userId": "08232317795619140301"
     },
     "user_tz": 420
    },
    "id": "mfMs3RBLZmGu",
    "outputId": "e0dbf0aa-f16b-4051-b925-0eb8af97f494"
   },
   "outputs": [],
   "source": [
    "!wget -q -O - https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_a1P6UmYmqD_",
   "metadata": {
    "id": "_a1P6UmYmqD_"
   },
   "source": [
    "### Click on the link ending with .loca.lt and paste the IP above to launch the Interface for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gohIFyv1ST9i",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gohIFyv1ST9i"
   },
   "outputs": [],
   "source": [
    "!streamlit run /content/app.py & echo y | npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
